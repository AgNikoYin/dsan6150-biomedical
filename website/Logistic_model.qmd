---
title: "Logistic Regression Model"
subtitle: "Group #6"
format:
  html:
    embed-resources: true
    code-fold: true
    warning: false
---


This page presents an exploration of survival status outcomes based on the analysis of patient data using Logistic Regression with different feature selection strategies. Dropping less significant features can avoid overfitting and reduce computational complexity and improve model reliability.

## Data Preparation

The dataset we are using in this analysis are the filtered and scaled version of the original dataset, we first scaled all  continues variables, and then set the landmark at 1000, drop all cid=0 to prevent issues of the unbalance dataset will bring to the prediction model.  

![](figures/eda/eda_plot_2.png)
At time = 1000, we have a similar sizes of patients with Cid=0 and Cid = 1.
The original dataset has 2139 observations, 24 variables, and the modifed data has 1006 observation.

Original Dataset:

| Dataset          | Cid= 0 | Cid= 1|
|------------------|--------|-------|
| Original Dataset |  1627  |  521  |
| Modified Dataset |  503   |  503  |


## Initial Model and Performance

The initial logistic regression model used all 22 features and target variable `cid`, which is the status of patients. We split the dataset into training and testing sets with 7:3 ratio.

This model achieved an accuracy of **72%**  on the test set and demonstrated strong discriminatory ability, significant predictors identified in the model included age, hemoglobin levels `hemo`, drug use `drugs`, and pre-antiretroviral treatment status `preanti`, among others.


![](figures/Logit/logit_plot_1.png)
The Initial mode has an AUC value of **0.7812**.

### fit with selected features with P<0.05

We choose features with p<0.05 and used them on the refine model. 
The refined model has an accuracy of **71%**, lower than the original model, thus we reject this model.


## Lasso Regression

We employed the Lasso regression to enhance feature selection and improve model interpretability.

The Lasso regression applies L1 penalty, set less relevant features with a coefficient of 0. Using cross-validation, the optimal penalty parameter (lambda) was determined to be the value that minimized the cross-validated error.
The features we choosed using lasso regression are `offtrt`,  `cd420`, `race`,  `symptom`, `drugs`,`treat`, `cd820`,  `strat`.

The model has an accuracy of **72.67%**, and AUC value of **0.797**. 
![](figures/Logit/logit_plot_2.png)

The Lasso model has a better performance than the initial logistic regression model with better discriminative power.


## Backward Selection

We then apply backward selection for feature selection starting from the full model. The process will drop features with the highest p-values and retain only statistically significant predictors.
The features we kept are: `karnof`, `symptom`, `offtrt`, `cd420`, `cd820`.

The model after using these features has a accuracy of **74.67%** 
with 95% CI : (0.7036, 0.8118). 
![](figures/Logit/logit_plot_3.png)
The model also has an AUC value of **0.7924**.

Backward selection give the best performance out of all three models     .

## Conclusion

![](figures/Logit/logit_plot_4.png)
In Logistic Regression section, we find that the model using backward selection has the best performance as it achieved the highest accuracy (**77.25%**) and the AUC value (**0.8523**) among all 3 models.

| Predictor   | Coefficient | p-value        |
|-------------|-------------|----------------|
| (Intercept) | 3.47756     | 0.018162 *     |
| karnof      | -0.03901    | 0.010080 *     |
| symptom     | 0.58024     | 0.014384 *     |
| treat       | -0.68194    | 0.000907 ***   |
| offtrt      | 1.23530     | 1.12e-09 ***   |
| cd420       | -1.09290    | < 2e-16 ***    |
| cd80        | 0.13870     | 0.124861       |

`offtrt` and `cd420` are strong predictors of survival, showing a negative impact when their respective conditions worsen.
`cd820` show positive associations with survival, highlighting potential protective effects or favorable prognostic indicators.
These insights could inform clinical interventions by emphasizing the need to prioritize treatment adherence and monitor biomarkers like `cd420` and `cd820`.

The Lasso has a confusion matrix of this:

| Prediction | Actual | 0  | 1 |
|---|---|---|----|
|          0 |  |  120  | 46 |
|          1 |  | 30 | 104 |



The lasso balanced the predictive power and feature selection through regularization with better AUC. while backward selection giving a slightly less complex model with better accuracy. The confusion matrix also validate the accuracy was not affected by the high ratio between cid=1 and cid =0 as we filtered the data earlier.


